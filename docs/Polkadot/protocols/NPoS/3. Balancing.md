---
title: Computing a balanced solution
---
![](Computed-balanced-solution.png)

In this technical note, we propose two algorithmic approaches to solving the so-called *balancing problem*, which arises within the validator election protocol under NPoS, specifically when a committee of validators has already been elected. To identify the most effective method for assigning nominatorsâ€™ stake to these validators, we examine (1) parametric flow algorithms and (2) a heuristic known as star balancing, followed by a comparative analysis of the two.

To follow the rationale effectively, the reader must be familiar with our [research paper](2.%20Paper.md), particularly the concept of balanced solutions defined therein. Although that paper proves that balanced solutions can be computed efficiently, it provides limited details on the underlying procedures. These details are elaborated in this note.

Let us then begin by establishing some notation. 

## 1. Notation

We begin with an instance of NPoS modelled as a bipartite graph $(N\cup A, E)$, where $N$ is the set of nominators and $A$ is a committee of elected validators of size $k$, with $k:=|A|\ll |N|$. In this setup, an edge $nv\in E$ exists whenever nominator $n$ endorses validator $v\in A$. 

We are also given a vector $s\in\mathbb{R}^N_{\geq 0}$, where $s_n$ denotes the stake of nominator $n$. An edge weight vector $w\in \mathbb{R}^E_{\geq 0}$ is called  _feasible_ if it is component-wise non-negative and satisfies the constraint: $\sum_{v\in A: \ nv\in E} w_{nv} \leq s_n$ for each nominator $n\in N$. We say that $w$ is _tight_ if the inequality is tight for every nominator $n$ that has at least one neighbor in $A$.

Let $B\in \{0,1\}^{A\times E}$ be the node-edge incidence matrix for the validator set $A$. For any weight vector $w\in \mathbb{R}_{\geq 0}^E$, the total support assigned by $w$ to each validator in $A$ is given by the vector $supp_w :=Bw\in \mathbb{R}^A$. For any validator $v\in A$, the support is given by 
$$supp_w(v)=(Bw)_v = \sum_{n\in N: \ nv\in E} w_{nv}$$, which represents the total amount of stake that $w$ allocates to $v$ from the nominators. 

Given an instance as described above, the *balancing problem* consists of finding a tight vector $w$ that minimizes the squared $\ell_2$ norm of its support vector, that is, minimizing the following expression:

$$val(w):= \|supp_w\|^2 = \|Bw\|^2.$$

An optimal solution to this problem corresponds exactly to a balanced solution, as defined in our paper.

## 2. Algorithms

To address the balancing problem, we can consider three different approaches:

1. **Convex programming**: The problem can be solved using numerical methods for convex quadratic programs. This approach is too computationally expensive to pursue further.
2. **Parametric flow algorithms**: As shown in the [research paper](2.%20Paper.md), the balancing problem can potentially be solved in time $O(|E|k + k^3)$ using advanced techniques for parametric flow problems. 
3. **Simple combinatorial heuristic**: The *star balancing* heuristic begins with any tight vector $w$ and converges to an optimal vector $w^*$ by applying a local weight-balancing rule. It runs in time $\tilde{O}(|E|k^2)$, ignoring logarithmic factors.

Although the worst-case complexity bound appears more favorable for technique 2 than for technique 3, this may not be entirely accurate in practice. As discussed by [Babenko et al. (2007)](https://pdfs.semanticscholar.org/6f44/e6d773cb5093e441f0370b1ec9dd047a5c24.pdf), who studied a parametric max flow problem closely related to the balancing problem, both techniques were experimentally evaluated using real-world and synthetic data. Their application focused on revenue optimization, and their results showed that the performance of star balancing is actually comparable to that of parametric flow algorithms, except in cases involving degenerate graph topologies. In fact, they conjecture that the two techniques exhibit similar complexities when the underlying graph has reasonably good expansion properties.

In light of this, and given that star balancing is significantly easier to implement than the parametric flow-based algorithm, we recommend using star balancing for NPoS. 


## 3. The star balancing heuristic

Star balancing is a combinatorial randomized algorithm that yields a solution arbitrarily close to optimal with high probability. This qualifies as a polynomial-time randomized approximation scheme (PRAS). An alternative analysis to this algorithm can be found in [Tarjan et al. (2006)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.7945&rep=rep1&type=pdf). We establish the following result:

__Theorem:__ For any fixed parameters $\varepsilon, \delta>0$, the star balancing algorithm outputs a tight weight vector $w$ such that, with probability at least $(1 - \delta)$, its value $val(w)$ lies within a multiplicative factor of at most $(1+\varepsilon)$ from the minimum. The algorithm runs in time 
$$O(|E|k^2 \log (k/\varepsilon \delta)).$$

__Algorithm: Star balancing.__

Consider the instance $(N\cup A, E, s)$, where each nominator $n\in N$ has a set of neighbors $A_n\subseteq A$ in the validator committee.

Fix constants $\varepsilon, \delta>0$. The algorithm starts with an arbitrary tight vector $w$, and iteratively refines it over $r$ rounds, where we will later specifiy the value of $r$ and show that $r = O(|N|k^2\log(k/\varepsilon \delta))$. 

**Algorithm Steps:**

1. **Initialization**: Find any tight vector $w$.

2. **Iterative refinement** (repeat $r$ times): a. Select a nominator $n\in N$ uniformly at random. b. Update the weights of the edges incident to $n$, maintaining tightness and non-negativity, so as to balance the support among its neighboring validators. Specifically ensure that:

    $$\forall v,v'\in A_n, \ supp_w(v)>supp_w(v') \rightarrow w_{nv}=0.$$

3. **Output**: Return the final tight vector $w$.

__Running time:__ After a single round of the algorithm, if nominator $n$ is selected, the running time of that round is $O(|A_n|)$, assuming that floating-point arithmetic operations take constant time. The expected running time per round is thus proportional to $\frac{1}{|N|}\sum_{n\in N} |A_n|=\frac{|E|}{|N|}$. Combining this with the bound on the number of rounds $r$, the total running time of the algorithm is 
$$O(r|E|/|N|) = O(|E|k^2\log(k/\varepsilon \delta)).$$

__Analysis:__ Let $w^0$ be the initial weight vector and $w^*$ the optimal solution. For each round $i\leq r$, denote by $w^i$ the state of $w$ at the end of the $i$-th round. We begin with a single observation.


__Lemma 1:__ $val(w^0)\leq k\cdot val(w^*)$.

_Proof:_ The objective value to minimize is $val(w)=\|Bw\|^2_2=\|supp_w\|_2^2$. Since both $w^0$ and $w^*$ are tight, their support vectors have equal $\ell_1$ norms. Therefore, 
$$val(w^0)=\|Bw^0\|_2^2 \leq \|Bw^0\|_1^2 
= \|Bw^*\|_1^2 \leq k\cdot \|Bw^*\|_2^2 
= k\cdot val(w^*).$$

$$
\tag{$\blacksquare$}
$$
<br/>
<br/>

The next step is to show that, in expectation, the progress in objective value achieved during each round is proportional to the difference between the current value and the optimal one.

__Lemma 2:__ For each round $i\in\{1,\cdots,r\}$, with initial vector $w^{i-1}$ and final vector $w^i$, the expected value of the objective function satisfies 
$$val(w^{i-1}) - \mathbb{E}[val(w^{i})] \geq \frac{1}{k^2|N|} [val(w^{i-1}) - val(w^*)].$$

_Proof:_ Fix a round $i$, and for notational convenience, drop the superscripts $i$ and $i-1$ throughout this proof. Let $w$ denote the initial weight vector, and let $w'^n$ be the resulting vector when nominator $n$ is selected in that round. The expected improvement in the objective value is given by the average progress $\frac{1}{|N|}\sum_{n\in N} [val(w) - val(w'^n)]$. To establish a lower bound on this quantity, it suffices to define an alternative family of vectors $\{w^n\}_{n\in N}$ such that $val(w'^n)\leq val(w^n)$ for each $n$. We can then analyze the average improvement obtained by transitioning from $w$ to a corresponding vector in this family. 

To proceed, we define the vector $f:=w-w^*\in\mathbb{R}^E$. The following technical observation is essential, though its proof will be deferred. 

__Lemma 3:__ $\|f\|^2 \leq k^2 \|Bf\|^2.$

Consider the decomposition of vector $f$ as $f=\sum_{n\in N} f^n$, where each $f^n$ denotes the restriction of $f$ to the edges incident to nominator $n$. Define the family of weight vectors $\{w^n:= w-\frac{1}{k^2} f^n\}_{n\in N}$. Then $val(w'^n) \leq val(w^n)$ holds for all $n\in N$, as desired. This follows from the construction in step 2.b of the algorithm: $w'^n$ is the tight, maximally affordable vector minimizing the objective among all vectors differing from $w$ only on edges incident to $n$. All that remains is to bound the average progress in objective value with respect to the newly defined family.

For a fixed $n\in N$, we have 

$$\begin{align}
val(w) - val(w^n) &= \|Bw\|^2 - \|B(w-\frac{1}{k^2} f^n)\|^2 \\
& = \frac{2}{k^2} (Bw)^\intercal Bf^n - \frac{1}{k^4} \|f^n\|^2. 
\end{align}$$

Thus, the average progress over all $n\in N$ is

$$\begin{align}
\frac{1}{|N|}\sum_{n\in N} [val(w)-val(w^n)] 
&= \frac{2}{k^2|N|}(Bw)^\intercal B(\sum_{n\in N}f^n) - \frac{1}{k^4|N|}\sum_{n\in N}\|f^n\|^2 \\
&= \frac{1}{k^2|N|}[2(Bw)^\intercal Bf - \frac{1}{k^2} \|f\|^2] \\
&\geq \frac{1}{k^2|N|}[2(Bw)^\intercal Bf - \|Bf\|^2] \\
& = \frac{1}{k^2|N|} (Bf)^\intercal B(2w-f) \\
&= \frac{1}{k^2|N|} [B(w-w^*)]^\intercal B(w+w^*) \\
&= \frac{1}{k^2|N|} [ \|Bw\|^2 - \|Bw^*\|^2] \\
&= \frac{1}{k^2|N|} [val(w) - val(w^*)],
\end{align}$$

where the inequality comes from Lemma 3. This completes the proof of Lemma 2.
$$
\tag{$\blacksquare$}
$$
<br/>
<br/>

_Proof of Lemma 3:_ The vector $f$ can be interpreted as a flow over the network $(N\cup A, E)$. Since both $w$ and $w^*$ are tight, flow preservation holds at all nominators. Let $A_s, A_t\subseteq A$ denote the sets of sources and sinks, respectively. That is, the subsets of validators with net excess and net demand. By the flow decomposition theorem, the flow *f* can be expressed as a sum of single-source subflows $f=\sum_{v\in A_s} f^v$, where each $f^v$ originates from a single source validator. This decomposition generates no cycles by suitably adjusting the choice of the optimal solution $w^*=w-f$.

The edge support of each subflow $f^v$ resembles a directed acyclic graph (DAG) rooted at the single source node $v$. Edges on this DAG are organized by levels, where the level of an edge is defined by the length of the longest path from $v$ that includes it. These levels begin at 1 for edges incident to $v$, and go up to at most $2k$, since any simple path alternates between nominators and validators, with only $k$ validators. We now decompose $f^v$ by levels as $f^v=\sum_{i\leq 2k} f^{v,i}$, where each $f^{v,i}$ is the restriction of $f^v$ over edges at level $i$. Because node $v$ is the only source of excess, quantified by $supp_w(v)-supp_{w^*}(v)=(Bf)_v$, and all other nodes in the DAG preserve flow, the total weight along any level satisfies $i$ is $\|f^{v,i}\|_1 \leq (Bf)_v$. It follows that:  
$$\|f^v\|_2^2 = \sum_{i\leq 2k}\|f^{v,i}\|_2^2 
\leq \sum_{i\leq 2k} \|f^{v,i}\|_1^2 
\leq 2k\cdot (Bf)^2_v.$$

Putting things together, we get

\begin{align}
\|f\|^2_2 &= \|\sum_{v\in A_s} f^v\|_2^2 \\ 
&\leq |A_s|\sum_{v\in A_s} \|f^v\|_2^2 \\
& \leq 2k|A_s|\sum_{v\in A_s} (Bf)_v^2 \\ 
&= 2k|A_s|\cdot \|Bf\|_2^2, \\
\end{align}

where the first inequality is an application of a Cauchy-Schwarz inequality. 

Similarly, by considering sinks instead of sources, we obtain the bound $\|f\|^2 \leq 2k|A_t| \cdot \|Bf\|^2$. Summing this with the previous bound and dividing by two, yields
$$\|f\|^2 \leq k(|A_s|+|A_t|) \cdot \|Bf\|^2 \leq k^2 \|Bf\|^2,$$
which establishes the claim.
$$
\tag{$\blacksquare$}
$$
<br/>
<br/>

For each round $i\leq r$, let the random variable $\Delta^i:= val(w^i) - val(w^*)$ denote the deviation of the current solution from optimality in terms of objective value. We now refer back to Lemma 2 to demonstrate that $\Delta^i$ decreases exponentially in expectation. 

__Lemma 4:__ For any round $0<i\leq r$, the expected deviation from optimality satisfies
$$\mathbb{E}[\Delta^i] \leq k\cdot (1-\frac{1}{k^2|N|})^i val(w^*).$$

_Proof:_ Rewriting Lemma 2 yields $\mathbb{E}[\Delta^i]\leq (1-\frac{1}{k^2|N|}) \Delta^{i-1}$. By induction and linearity of expectation, it follows that $\mathbb{E}[\Delta^i]\leq (1-\frac{1}{k^2|N|})^i \Delta^0$. Finally, from Lemma 1 we know that $\Delta^0 = val(w^0) - val(w^*) < k\cdot val(w^*)$, which completes the bound. 
$$
\tag{$\blacksquare$}
$$
<br/>
<br/>

Since the value of the final output $val(w^r)$ must lie within a factor of $(1+\varepsilon)$ of the optimal value $val(w^*)$ with probability at least $(1-\delta)$, the next lemma completes the algorithm's analysis and establishes the main theorem.

__Lemma 5:__ If $r=\lceil |N|k^2\ln(k/\epsilon \delta) \rceil$, then $\mathbb{P}[val(w^r) > (1+\varepsilon)val(w^*)]\leq \delta$.

_Proof:_ By Lemma 4 and the choice of $r$, it follows that 
$$\mathbb{E}[\Delta^r]\leq \epsilon\cdot \delta\cdot val(w^*).$$

Since $\Delta^r$ is non-negative, we apply Markov's inequality:

$$\delta \geq \mathbb{P}[\Delta^r > \frac{\mathbb{E}[\Delta^r]}{\delta}]\geq \mathbb{P}[\Delta^r > \epsilon\cdot val(w^*)] 
= \mathbb{P}[val(w^r) > (1+\epsilon)\cdot val(w^*)],$$
and thus we prove the claim.
$$
\tag{$\blacksquare$}
$$
<br/>
<br/>

**For inquieries or questions, please contact**
